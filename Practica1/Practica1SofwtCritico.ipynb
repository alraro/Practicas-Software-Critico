{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6537b9",
   "metadata": {},
   "source": [
    "# Practica 1 Alfonso Ramos Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944db8a5",
   "metadata": {},
   "source": [
    "## Carga de librerias\n",
    "\n",
    "- **Numpy (np)**: operaciones numéricas y manipulación de arrays; aquí se usa para crear ventanas temporales (np.lib.stride_tricks.sliding_window_view) y cálculos vectorizados.\n",
    "- **Pandas (pd)**: lectura y manejo de datos tabulares/series temporales (pd.read_csv, index datetime, selección de columnas).\n",
    "- **Matplotlib.pyplot (plt)**: visualización de series temporales y marcado de anomalías (plot, scatter, show).\n",
    "- **Tensorflow (tf)**: framework de deep learning y backend para Keras.\n",
    "- **keras.models.Sequential**: contenedor secuencial para construir la RNN/modelo.\n",
    "- **keras.layers.LSTM**: capa recurrente LSTM para modelado de dependencias temporales.\n",
    "- **keras.layers.Dense**: capa densa/final para producir la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ded351",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Input, RepeatVector, TimeDistributed\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c4757",
   "metadata": {},
   "source": [
    "## 1. Contrucción de modelo LSTM y detección de anomalías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f3b19",
   "metadata": {},
   "source": [
    "### a. Construcción del modelo básico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d06399",
   "metadata": {},
   "source": [
    "#### Cargar datos\n",
    "Cargamos todo desde el csv de datos dado en el dataframe de pandas y escalamos los valores a un rango de [0,1], lo que mejorara la eficiencia de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aa7a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datos.csv\",parse_dates=True)\n",
    "print(\"Datos cargados: \\n\", df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aaabb9",
   "metadata": {},
   "source": [
    "#### Creamos las ventanas temporales\n",
    "Aquí el parametro principal es el tamaño de ventana, que varía mucho el resultado del modelo. En un principio probé con valores bajos (3, 5) y tras hacer pruebas, he descubierto que da mejores resultados tamaños medios de ventana de ventana. Primero con 24 daba mejores resultados, pero con 12 valores por ventana da los mejores resultados para nuestros datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f255f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 12\n",
    "windows = np.lib.stride_tricks.sliding_window_view(df[\"value\"], window_shape=(n_steps))\n",
    "print(\"Dimensiones de ventanas: \", windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe898651",
   "metadata": {},
   "source": [
    "#### Creamos datos de entrenamiento\n",
    "Usamos como datos todas las ventanas menos la última para la predicción.\n",
    "Para los valores objetivos usamos el primer valor siguiente a la ventana claro, que es el que puede predecir la primera ventana.\n",
    "Además de eso redimensionamos las ventanas para que las acepte el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed61f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = windows[:-1]\n",
    "y = df[\"value\"].values[n_steps:]\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "print(\"X Shape: \", X.shape)\n",
    "print(\"Y Shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc7fa1b",
   "metadata": {},
   "source": [
    "#### Creamos el modelo\n",
    "Creamos el modelo, con una capa de LSTM unida con una capa densa de 1 neurona al final como salida.\n",
    "En primer lugar, las neuronas de la capa LSTM dan mucho mejor resultado siendo 100 en vez de 50\n",
    "\n",
    "Además la funcion de activación sigmoide da unos resultados muy superiores a la tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65278bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "\tLSTM(100, activation='sigmoid', input_shape=(n_steps, 1)),\n",
    "\tDense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f5e10",
   "metadata": {},
   "source": [
    "#### Entrenamos el modelo con las ventanas\n",
    "Al principio usaba 50 epochs en el modelo, pensando que asi mejoraria mucho la calidad del modelo cuantos mas epochs use, sin embargo probando valores mas bajos, resulta que con tantisimos epochs se producia muchisimo sobre ajuste y además detectaba demasiados puntos como anomalías. Probando he encontrado que el numero óptimo de epochs es 15, manteniendo un mse medio final del modelo de aprox. 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274035d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "\n",
    "history = model.fit(X, y, epochs=epochs, verbose=1, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "\n",
    "print(\"Modelo entrenado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb23301",
   "metadata": {},
   "source": [
    "#### Graficamos la perdida para optimizar el modelo\n",
    "Este gráfico lo he estado usando para comprobar el numero optimo de iteraciones del modelo. Podemos observar que para el epoch 15 la pérdida no baja casi nada, por lo que no merece la pena seguir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aeecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.history.history['loss'], label='Pérdida de entrenamiento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida (MSE)')\n",
    "plt.title('Pérdida de Entrenamiento del Modelo LSTM')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd2a1d",
   "metadata": {},
   "source": [
    "#### Creamos los valores de prediccion\n",
    "Creamos las predicciones y calculamos el error de cada predicción con respecto al valor real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e17d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X)\n",
    "errors = y - predictions.flatten()\n",
    "print(\"Errores: \\n\", errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f8a08",
   "metadata": {},
   "source": [
    "### b. Detección de anomalías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a7904",
   "metadata": {},
   "source": [
    "#### Threshold y deteccion de anomalias\n",
    "Aquí usando el **rango intercuartílico** y usando el errror de la predicción usado antes, marcamos todos los puntos que esten 1.5 veces alejados de dicho rango como anomalías. Valores mayores de 1.5 provocaban demasiados falsos positivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7704eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRQ_DIFF = 1.5\n",
    "\n",
    "error_IQR = np.percentile(errors, 75) - np.percentile(errors, 25)\n",
    "print(\"Rango Intercuartil de los errores: \", error_IQR)\n",
    "anomalies = np.where(np.abs(errors) > IRQ_DIFF * error_IQR)[0]\n",
    "print(\"Número de anomalías detectadas: \", len(anomalies))\n",
    "error_df = df.iloc[anomalies]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6217741",
   "metadata": {},
   "source": [
    "#### Graficamos anomalias\n",
    "Graficamos los valores originales junto a las anomalias para observar el resultado final de nuestro modelo.\n",
    "\n",
    "Observamos que la anomalía principal en el centro del grafico la detecta sin ninguna duda, y pequeños picos anómalos al final de la gráfica también. No detecta ciertos valores que no son demasiado anómalos como los del principio de la gráfica, pero el modelo tiene un buen desempeño para ver anomalías grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02fbb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"El numero de anomalias es {len(anomalies)} de {len(df)}\")\n",
    "print(df.iloc[anomalies])\n",
    "plt.plot(df['timestamp'], df['value'], label='Valor real', color='blue')\n",
    "plt.plot(predictions, linestyle='dashed', color='orange', label='Predicciones')\n",
    "plt.scatter(\n",
    "    df['timestamp'][anomalies],\n",
    "    df['value'][anomalies],\n",
    "    color='red',\n",
    "    label='Anomalías')\n",
    "plt.xticks(df['timestamp'][::len(df)//10], rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27206253",
   "metadata": {},
   "source": [
    "### c. Mejora del modelo\n",
    "Los cambios que he aplicado para mejorar el modelo son los siguientes:\n",
    "\n",
    "Principales:\n",
    "- Reducir numero de epochs 50 -> 15\n",
    "- Aumentar tamaño de ventana 3 -> 12\n",
    "\n",
    "Adicionales:\n",
    "- Neuronas LSTM 50 -> 100\n",
    "- Funcion de activación 'tanh' -> 'sigmoid'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c59510",
   "metadata": {},
   "source": [
    "## 2. Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f125c9a0",
   "metadata": {},
   "source": [
    "#### Carga de datos\n",
    "- Cargamos los datos como antes. \n",
    "- Creamos una division para tener datos de entrenamiento y de validacion (Entrenamiento 80% - Validación 20%).\n",
    "- Escalar los valores a un rango de [0,1] para mejorar nuestro modelo, ya que mejoraba drasticamente el resultado final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0877edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datos.csv\",parse_dates=['timestamp'], index_col='timestamp')\n",
    "df = df.sort_index()\n",
    "\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_df, test_df = df.iloc[:train_size].copy(), df.iloc[train_size:].copy()\n",
    "\n",
    "min_train, max_train = train_df['value'].min(), train_df['value'].max()\n",
    "range_train = max_train - min_train\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scaler.fit(train_df[['value']])\n",
    "\n",
    "train_df['value_scaled'] = scaler.transform(train_df[['value']])\n",
    "test_df['value_scaled'] = scaler.transform(test_df[['value']])\n",
    "\n",
    "print(f\"Datos de entrenamiento: {len(train_df)}, Datos de prueba: {len(test_df)}\")\n",
    "\n",
    "print(\"Datos Entrenamiento: \\n\", train_df.head(5))\n",
    "print(\"Datos Prueba: \\n\", test_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa6364",
   "metadata": {},
   "source": [
    "#### Creacion de ventanas temporales\n",
    "- Creamos ventanas diferentes para el entrenamiento y validacion.\n",
    "- En este caso tras varias pruebas tambien me ha funcionado muy bien un tamaño de ventana de 12, consiguiendo una perdida de 0.2 mae aprox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c7b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 12\n",
    "\n",
    "train_data = train_df['value_scaled'].values\n",
    "test_data = test_df['value_scaled'].values\n",
    "\n",
    "X_train = np.lib.stride_tricks.sliding_window_view(train_data, window_shape=(n_steps)).copy()\n",
    "X_test = np.lib.stride_tricks.sliding_window_view(test_data, window_shape=(n_steps)).copy()\n",
    "\n",
    "print(\"Dimensiones X_train original: \", X_train.shape)\n",
    "print(\"Dimensiones X_test original: \", X_test.shape)\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "y_train = X_train\n",
    "y_test = X_test\n",
    "\n",
    "\n",
    "print(\"\\n--- Dimensiones para el modelo ---\")\n",
    "print(\"X_train Shape: \", X_train.shape)\n",
    "print(\"y_train Shape: \", y_train.shape)\n",
    "print(\"X_test Shape: \", X_test.shape)\n",
    "print(\"y_test Shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ea050",
   "metadata": {},
   "source": [
    "#### Creamos el autoencoder\n",
    "- He decidido usar un cuello de botella de 16, ya que en un principio puse 50 y descubir que habia que usar uno mucho mas pequeño para mi caso concreto\n",
    "- Importante tambien el cambio al mae con respecto al ejercicio 1, este es mucho mejor para no ajustar el modelo a los outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baa554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOTTLENECK_SIZE = 16\n",
    "\n",
    "inputs = Input(shape=(n_steps, 1))\n",
    "encoded = LSTM(BOTTLENECK_SIZE, activation='tanh')(inputs)\n",
    "decoded = RepeatVector(n_steps)(encoded)\n",
    "decoded = LSTM(BOTTLENECK_SIZE, activation='tanh', return_sequences=True)(decoded)\n",
    "decoded = TimeDistributed(Dense(1))(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=inputs, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6fc928",
   "metadata": {},
   "source": [
    "#### Entrenamos el modelo\n",
    "Ajustamos el modelo con los datos de prueba y validacion y en este caso si que merece la pena usar 50 epochs. La diferencia de tener un conjunto de datos de validación se nota muchisimo en el resultado final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a8c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "history = autoencoder.fit(X_train, y_train,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=epochs, batch_size=32,\n",
    "                        shuffle=False,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0373a3af",
   "metadata": {},
   "source": [
    "#### Predictions\n",
    "- Creo las predicciones del modelo de datos y validacion, y creo nuevos dataframes para guardar los resultados de las predicciones, y si son anomalias o no.\n",
    "- He cambiado el criterio de detección de nomalías a un percentil del 99%, en vez del rango intercuartílico, y tambien da buenos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8731573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = autoencoder.predict(X_train)\n",
    "predictions_test = autoencoder.predict(X_test)\n",
    "\n",
    "mae_train = np.mean(np.abs(predictions_train - X_train), axis=(1,2))\n",
    "mae_test = np.mean(np.abs(predictions_test - X_test), axis=(1,2))\n",
    "\n",
    "print(\"\\n--- Formas de MAE ---\")\n",
    "print(\"MAE Train: \", mae_train.shape)\n",
    "print(\"MAE Test: \", mae_test.shape)\n",
    "\n",
    "threshold = np.percentile(mae_train, 99)\n",
    "\n",
    "print(f\"Umbral de MAE para anomalías (99 percentil): {threshold:.4f}\")\n",
    "\n",
    "pred_last_step_train = predictions_train[:, -1, :]\n",
    "pred_last_step_test = predictions_test[:, -1, :]\n",
    "\n",
    "pred_original_scale_train = scaler.inverse_transform(pred_last_step_train)\n",
    "pred_original_scale_test = scaler.inverse_transform(pred_last_step_test)\n",
    "\n",
    "train_scores_df = pd.DataFrame(index=train_df.index[n_steps-1:])\n",
    "train_scores_df['mae'] = mae_train\n",
    "train_scores_df['threshold'] = threshold\n",
    "train_scores_df['anomaly'] = train_scores_df['mae'] > train_scores_df['threshold']\n",
    "train_scores_df['value'] = train_df['value'][n_steps - 1:]\n",
    "train_scores_df['predicted'] = pred_original_scale_train\n",
    "\n",
    "test_scores_df = pd.DataFrame(index=test_df.index[n_steps - 1:])\n",
    "test_scores_df['mae'] = mae_test\n",
    "test_scores_df['threshold'] = threshold\n",
    "test_scores_df['anomaly'] = test_scores_df['mae'] > test_scores_df['threshold']\n",
    "test_scores_df['value'] = test_df['value'][n_steps - 1:]\n",
    "test_scores_df['predicted'] = pred_original_scale_test\n",
    "\n",
    "all_scores_df = pd.concat([train_scores_df, test_scores_df])\n",
    "\n",
    "print(\"\\nAnomalías en datos de entrenamiento:\", train_scores_df['anomaly'].sum())\n",
    "print(\"\\nAnomalías en datos de prueba:\", test_scores_df['anomaly'].sum())\n",
    "\n",
    "print(\"\\n--- Anomalías detectadas en datos de prueba ---\")\n",
    "print(test_scores_df[test_scores_df['anomaly']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a76a9",
   "metadata": {},
   "source": [
    "#### Graficamos anomalias\n",
    "Graficamos las anomalias de la misma manera que en el modelo anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db29a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_df = all_scores_df[all_scores_df['anomaly']]\n",
    "print(\"El numero de anomalias es \", len(anomalies_df), \" de \", len(all_scores_df))\n",
    "print(anomalies_df[['value']])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 8), sharex=True)\n",
    "\n",
    "ax.set_title(\"Detección de Anomalías\")\n",
    "ax.plot(\n",
    "    all_scores_df.index,\n",
    "    all_scores_df['value'],\n",
    "    label='Valor original',\n",
    "    color='blue'\n",
    ")\n",
    "ax.plot(\n",
    "    all_scores_df.index,\n",
    "    all_scores_df['predicted'],\n",
    "    label='Valor predicho',\n",
    "    color='orange',\n",
    "    linestyle='dashed'\n",
    ")\n",
    "ax.scatter(\n",
    "    anomalies_df.index,\n",
    "    anomalies_df['value'],\n",
    "    label='Anomalía',\n",
    "    color='red',\n",
    "    marker='o',\n",
    "    s=50\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_ylabel('Valor')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e1eb8",
   "metadata": {},
   "source": [
    "### Mejoras implementadas\n",
    "- He usado en este caso un MinMaxScaler, para poder escalar los valores para que el autoencoder funcione mejor, y despues poder desescalarlos y ver la prediccion real de mi modelo.\n",
    "- El tamaño de ventana lo he mantenido a 12, lo cual me ha dado muy buenos resultados en esta ocasión también\n",
    "- He usado el mae en vez de el mse, ya que este ultimo penalizaba al modelo mucho por las anomalias y no hacia buenas predicciones.\n",
    "- Y para el autoencoder he usado la funcion tanh predecia mejor, junto a un bottleneck de 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6052e2",
   "metadata": {},
   "source": [
    "## 3.Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5253a47",
   "metadata": {},
   "source": [
    "### Cargamos y preparamos los datos\n",
    "Aqui creo el dataframe como en los 2 ejercicios anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aae86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datos.csv\",parse_dates=['timestamp'], index_col='timestamp')\n",
    "df = df.sort_index()\n",
    "\n",
    "print(\"Datos cargados y ordenados: \\n\", df.head(5))\n",
    "print(f\"Total de datos: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f5c11",
   "metadata": {},
   "source": [
    "### Creamos las ventanas y preparamos df de resultados\n",
    "- Aqui creo las ventanas como siempre y creo el dataframe de resultados final\n",
    "- El tamaño de ventana lo he mantenido en 12 ya que sigue funcionando muy bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15727b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 12\n",
    "\n",
    "X_data = np.lib.stride_tricks.sliding_window_view(df[\"value\"], window_shape=(n_steps))\n",
    "\n",
    "print(\"Dimensiones de ventanas: \", X_data.shape)\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "\t'value': df['value'][n_steps - 1:],\n",
    "}, index=df.index[n_steps - 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662b54e",
   "metadata": {},
   "source": [
    "### Creamos modelo de IsolationForest\n",
    "- Los estimadores no he encontrado casi diferencia al aumentarlos en este caso, siemplemente tarda mas el modelo.\n",
    "- La contaminacion si que se nota mucho, con valores mas bajos como 0.05 o menos detectaba solo las anomalias mas exageradas. Con 0.1 detecta bastantes mas que son mas sutiles, y creo que en este contexto de un meidor de temperatura, no pasa nada por aumentar mas falsos positivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 100\n",
    "contamination = 0.1\n",
    "\n",
    "iso_forest = IsolationForest(n_estimators=n_estimators, contamination=contamination, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fcbf64",
   "metadata": {},
   "source": [
    "### Entrenamos y predecimos los valores del modelo\n",
    "- Entreno predigo y almaceno los valores que ha obtenido el IsolationForest, marcando si son anomalias o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa374369",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = iso_forest.fit_predict(X_data)\n",
    "scores = iso_forest.decision_function(X_data)\n",
    "\n",
    "iso_forest.save(\"isolation_forest_model.keras\")\n",
    "\n",
    "df_results['anomaly'] = predictions == -1\n",
    "df_results['anomaly_score'] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810bbe2",
   "metadata": {},
   "source": [
    "### Graficamos las anomalias detectadas\n",
    "Graficamos las anomalias y el grafico original como siempre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb6c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_df = df_results[df_results['anomaly']]\n",
    "print(f\"El numero de anomalias es {len(anomalies_df)} de {len(df_results)}\")\n",
    "print(anomalies_df[['value']])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 8), sharex=True)\n",
    "\n",
    "ax[0].set_title(\"Detección de Anomalías con Isolation Forest\")\n",
    "ax[0].plot(\n",
    "    df_results.index, \n",
    "    df_results['value'], \n",
    "    label='Valor original', \n",
    "    color='blue'\n",
    ")\n",
    "ax[0].scatter(\n",
    "    anomalies_df.index, \n",
    "    anomalies_df['value'], \n",
    "    label='Anomalía (detectada por IF)', \n",
    "    color='red', \n",
    "    marker='o',\n",
    "    s=50\n",
    ")\n",
    "ax[0].legend()\n",
    "ax[0].set_ylabel('Valor')\n",
    "\n",
    "ax[1].set_title(\"Score de Anomalía de Isolation Forest\")\n",
    "ax[1].plot(\n",
    "    df_results.index, \n",
    "    df_results['anomaly_score'], \n",
    "    label='Score de Anomalía', \n",
    "    color='green'\n",
    ")\n",
    "ax[1].scatter(\n",
    "    anomalies_df.index, \n",
    "    anomalies_df['anomaly_score'],\n",
    "    label='Anomalía',\n",
    "    color='red',\n",
    "    marker='o',\n",
    "    s=50\n",
    ")\n",
    "ax[1].legend()\n",
    "ax[1].set_ylabel('Score (más bajo = más anómalo)')\n",
    "ax[1].set_xlabel('Timestamp')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
